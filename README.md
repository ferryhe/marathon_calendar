# README.md

## Crawler Module

### Overview
The Crawler module is designed to automate the data collection process from various sources. It fetches, processes, and stores data efficiently.

### Features
- Automated data fetching
- Data processing pipelines
- Integration with existing schemas

### Schema Extensions
The following extensions have been made to enhance data structure compatibility:

- **New Fields**:
  - `source_url`: URL of the data source.
  - `last_crawled`: Timestamp of the last crawl operation.

- **Updated Fields**:
  - `data_format`: Changed to accept additional data formats (e.g., XML, JSON).

Original content preserved as requested.